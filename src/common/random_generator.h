/*
 * Licensed to the Apache Software Foundation (ASF) under one
 * or more contributor license agreements.  See the NOTICE file
 * distributed with this work for additional information
 * regarding copyright ownership.  The ASF licenses this file
 * to you under the Apache License, Version 2.0 (the
 * "License"); you may not use this file except in compliance
 * with the License.  You may obtain a copy of the License at
 *
 *   http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing,
 * software distributed under the License is distributed on an
 * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
 * KIND, either express or implied.  See the License for the
 * specific language governing permissions and limitations
 * under the License.
 */

/*!
 * Copyright (c) 2017 by Contributors
 * \file random_generator.h
 * \brief Native random number generator.
 */
#ifndef MXNET_COMMON_RANDOM_GENERATOR_H_
#define MXNET_COMMON_RANDOM_GENERATOR_H_

#include <mxnet/base.h>
#include <random>
#include <new>

#if MXNET_USE_CUDA
#include <cuda.h>
#include <cuda_runtime.h>
#include <curand_kernel.h>
#endif  // MXNET_USE_CUDA

using namespace mshadow;

namespace mxnet {
namespace common {
namespace random {

template<typename Device, typename DType MSHADOW_DEFAULT_DTYPE>
class RandGenerator;

// Elementary random number generation for int/uniform/gaussian in CPU and GPU.
// Will use float data type whenever instantiated for half_t or any other non
// standard real type.
template<typename Device, typename DType MSHADOW_DEFAULT_DTYPE>
class RandGeneratorImpl;

// at least how many random numbers should be generated by one CPU thread.
const int kCPUMinRndNumberPerThread = 64;
// store how many global random states for CPU.
const int kCPURndStateNum = 1024;

template<typename DType>
class RandGeneratorImpl<cpu, DType> {
 public:
  typedef typename std::conditional<std::is_floating_point<DType>::value,
                                    DType, double>::type FType;

  explicit RandGeneratorImpl<cpu, DType>(std::mt19937 *ptr_engine) : engine_(ptr_engine) {}

  MSHADOW_XINLINE int rand() { return engine_->operator()(); }

  MSHADOW_XINLINE FType uniform() {
    typedef typename std::conditional<std::is_integral<DType>::value,
                                      std::uniform_int_distribution<DType>,
                                      std::uniform_real_distribution<FType>>::type GType;
    GType dist_uniform;
    return dist_uniform(*engine_);
  }

  MSHADOW_XINLINE FType normal() {
    std::normal_distribution<FType> dist_normal;
    return dist_normal(*engine_);
  }

 private:
  std::mt19937 *engine_;
};

template<typename DType>
class RandGenerator<cpu, DType> {
 public:
  MSHADOW_XINLINE void dispose() {}

  MSHADOW_XINLINE RandGeneratorImpl<cpu, DType> Get(int idx = 0) {
    std::mt19937 *ptr_engine = &states_[idx];
    RandGeneratorImpl<cpu, DType> gen(ptr_engine);
    return gen;
  }

  MSHADOW_XINLINE void Seed(Stream<cpu> *, uint32_t seed) {
    for (int i = 0; i < kCPURndStateNum; ++i) (states_ + i)->seed(seed + i);
  }

  MSHADOW_XINLINE void set_state(int idx, const std::mt19937 &state) {
    states_[idx] = state;
  }

 private:
  std::mt19937 states_[kCPURndStateNum];
};

#if MXNET_USE_CUDA

// at least how many random numbers should be generated by one GPU thread.
const int kGPUMinRndNumberPerThread = 64;
// store how many global random states for GPU.
const int kGPURndStateNum = 32768;

// uniform number generation in Cuda made consistent with stl (include 0 but exclude 1)
// by using 1.0-curand_uniform().
// Needed as some samplers in sampler.h won't be able to deal with
// one of the boundary cases.
template<typename DType>
class RandGeneratorImpl<gpu, DType> {
 public:
  // Copy state to local memory for efficiency.
  __device__ explicit RandGeneratorImpl(curandStatePhilox4_32_10_t *state)
      : state_(*state) {}

  MSHADOW_FORCE_INLINE __device__ int rand() {
    return curand(&state_);
  }

  MSHADOW_FORCE_INLINE __device__ float uniform() {
    return static_cast<float>(1.0) - curand_uniform(&state_);
  }

  MSHADOW_FORCE_INLINE __device__ float normal() {
    return curand_normal(&state_);
  }

  MSHADOW_FORCE_INLINE __device__ curandStatePhilox4_32_10_t get_state() {
    return state_;
  }

 private:
  curandStatePhilox4_32_10_t state_;
};

template<>
class RandGeneratorImpl<gpu, double> {
 public:
  // Copy state to local memory for efficiency.
  __device__ explicit RandGeneratorImpl(curandStatePhilox4_32_10_t *state)
      : state_(*state) {}

  MSHADOW_FORCE_INLINE __device__ int rand() {
    return curand(&state_);
  }

  MSHADOW_FORCE_INLINE __device__ double uniform() {
    return static_cast<double>(1.0) - curand_uniform_double(&state_);
  }

  MSHADOW_FORCE_INLINE __device__ double normal() {
    return curand_normal_double(&state_);
  }

  MSHADOW_FORCE_INLINE __device__ curandStatePhilox4_32_10_t get_state() {
    return state_;
  }

 private:
  curandStatePhilox4_32_10_t state_;
};

template<typename DType>
class RandGenerator<gpu, DType> {
 public:
  RandGenerator() {
    cudaError_t e = cudaMalloc(&states_, kGPURndStateNum * sizeof(curandStatePhilox4_32_10_t));
    if (e != cudaSuccess && e != cudaErrorCudartUnloading) {
      throw std::bad_alloc();
    }
  }

  MSHADOW_FORCE_INLINE __device__ RandGeneratorImpl<gpu, DType> Get(int idx = 0) {
    curandStatePhilox4_32_10_t *ptr_state = states_ + idx;
    RandGeneratorImpl<gpu, DType> gen(ptr_state);
    return gen;
  }

  void Seed(Stream<gpu> *s, uint32_t seed);

  MSHADOW_FORCE_INLINE __device__ void set_state(int idx,
                                                 const curandStatePhilox4_32_10_t &state) {
    states_[idx] = state;
  }

  // Free the allocated GPU memory.
  // For global singleton,
  // calling this in destructor may cause undefined behavior.
  MSHADOW_FORCE_INLINE __host__ void dispose() {
    if (states_) {
      cudaError_t err = cudaFree(states_);
      if (err != cudaSuccess && err != cudaErrorCudartUnloading) {
        LOG(FATAL) << "CUDA: " << cudaGetErrorString(err);
      }
      states_ = nullptr;
    }
  }

 private:
  // sizeof(curandStatePhilox4_32_10_t) = 64
  // sizeof(curandState_t) = 48
  // while for a large amount of states, we observe
  // curand_init(curandState_t *) allocates extra memories on device,
  // (which is not mentioned in NVIDIA's documents).
  // Thus we decide to use curandStatePhilox4_32_10_t here.
  curandStatePhilox4_32_10_t *states_;
};

#endif  // MXNET_USE_CUDA

}  // namespace random
}  // namespace common
}  // namespace mxnet
#endif  // MXNET_COMMON_RANDOM_GENERATOR_H_
